{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストを数値の特徴ベクトルとして表現<br>\n",
    "1,ドキュメント集合全体から単語という一意なトークンからなる語彙を作成<br>\n",
    "2,各ドキュメントにおける各単語の出現回数を含んだ特徴ベクトルを作る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語を処理するためには、形態素解析が必要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流れ参考<br>\n",
    "https://qiita.com/yasunori/items/31a23eb259482e4824e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['python', 'MeCab', '使っ', '形態素', '解析', '行う'],\n",
       " ['あなた', '空腹'],\n",
       " ['お前', '空腹', 'の']]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "##形態素を品詞指定して行う\n",
    "def extractKeyword(text):\n",
    "    tagger = MeCab.Tagger('-Ochasen')\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(text)\n",
    "    #node = tagger.parse(text)\n",
    "    keywords = []\n",
    "    while node:\n",
    "        if node.feature.split(\",\")[0] == u\"名詞\":\n",
    "            keywords.append(node.surface)\n",
    "        #elif node.feature.split(\",\")[0] == u\"形容詞\":\n",
    "        #     keywords.append(node.surface)\n",
    "        elif node.feature.split(\",\")[0] == u\"動詞\":\n",
    "             keywords.append(node.surface)\n",
    "        node = node.next\n",
    "    return keywords\n",
    "\n",
    "\n",
    "text = [\"pythonでMeCabを使って形態素解析を行う。\" , 'あなたは空腹なの？' , 'お前も空腹なのか']\n",
    "vocab = []\n",
    "for i in range(len(text)):\n",
    "    #vocab.append(extractKeyword(text[i]))\n",
    "    L = extractKeyword(text[i])\n",
    "    #joined_text = ' '.join(L)\n",
    "    #vocab.append(joined_text)\n",
    "    vocab.append(L)\n",
    "\n",
    "vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': 1, 'mecab': 0, '使っ': 4, '形態素': 5, '解析': 8, '行う': 7, 'あなた': 2, '空腹': 6, 'お前': 3}\n",
      "[[1 1 0 0 1 1 0 1 1]\n",
      " [0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "##特徴ベクトルに変換\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "##↑引数にstop_words = ['' , '',,,,,,,]のように与える\n",
    "docs = np.array(vocab)\n",
    "\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "##単語ごとのid\n",
    "print(count.vocabulary_)\n",
    "\n",
    "\n",
    "##単語ごとのidに対する出現回数\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41 0.41 0.   0.   0.41 0.41 0.   0.41 0.41]\n",
      " [0.   0.   0.8  0.   0.   0.   0.61 0.   0.  ]\n",
      " [0.   0.   0.   0.8  0.   0.   0.61 0.   0.  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "##tfidfに変換\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf = True , norm='l2' , smooth_idf = True)\n",
    "##少数第2位まで\n",
    "np.set_printoptions(precision = 2)\n",
    "\n",
    "##tfidfの分散表現\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfid = tfidf.fit_transform(count.fit_transform(docs)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "###最大のtfidfの値をとるインデックスを返す\n",
    "\n",
    "m = []\n",
    "for i in range(tfid.shape[0]):\n",
    "    arg= np.argmax(tfid[i])\n",
    "    m.append(arg)\n",
    "print(m)\n",
    "\n",
    "##mが最大tfidを取る箇所→最大単語idを取っている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "メモリ解放しておく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "お試し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "{'pythonでmecabを使って形態素解析を行う': 0, 'あなたは空腹なの': 1, 'お前も空腹なのか': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_countvectorizer_from_texts(texts):\n",
    "    count = CountVectorizer()\n",
    "    docs = np.array(texts)\n",
    "    bags = count.fit_transform(docs)\n",
    "    print(bags.toarray())\n",
    "    print(count.vocabulary_)\n",
    "\n",
    "texts = [\"He likes to play the guitar\", \\\n",
    "\"She likes to play the piano\", \\\n",
    "\"He likes to play the guitar, and she likes to play the piano\"]\n",
    "create_countvectorizer_from_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['python', 'MeCab', '使っ', '形態素', '解析', '行う']\\t['あなた', '空腹']\\t['お前', '空腹', 'の']\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\t'.join([str(i) for i in vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "お試し2 n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://hayataka2049.hatenablog.jp/entry/2017/02/06/215619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "##文章をリスト化\n",
    "data_lst = [\"吾輩は猫である\",\n",
    "            \"国境の長いトンネルを抜けると雪国であった\",\n",
    "            \"恥の多い生涯を送って来ました\",\n",
    "            \"一人の下人が、羅生門の下で雨やみを待っていた\",\n",
    "            \"幼時から父は、私によく、金閣のことを語った\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def ngram_split(string, n, splitter=\"-*-\"):\n",
    "    \"\"\"\n",
    "    string:iterableなら何でも n:n-gramのn　splitter:処理対象と混ざらなければ何でも良い\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    for i in range(len(string[:-n+1])):\n",
    "        lst.append(splitter.join(string[i:i+n]))\n",
    "    return lst\n",
    "\n",
    "\n",
    "def itr_dict(itr):\n",
    "    d = defaultdict(int)\n",
    "    for x in itr:\n",
    "        d[x] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "bgram_dict_lst = [itr_dict(ngram_split(x, 3)) for x in data_lst]\n",
    "dict_vectorizer = DictVectorizer()\n",
    "a = dict_vectorizer.fit_transform(bgram_dict_lst).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_array = np.array([[len(x)]*a.shape[1] for x in data_lst])\n",
    "#↑もうちょっと綺麗な方法があるなら教えて欲しい\n",
    "\n",
    "std_a = a/len_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 74)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 74)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "お試し3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a pen. I live in Osaka\n"
     ]
    }
   ],
   "source": [
    "devided_text = ['This is a pen.', 'I live in Osaka']\n",
    "joined_devided_text = ' '.join(devided_text)\n",
    "print(joined_devided_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://qiita.com/mergit/items/822dc49343c887019d44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# size: 圧縮次元数\n",
    "# min_count: 出現頻度の低いものをカットする\n",
    "# window: 前後の単語を拾う際の窓の広さを決める\n",
    "# iter: 機械学習の繰り返し回数(デフォルト:5)十分学習できていないときにこの値を調整する\n",
    "# model.wv.most_similarの結果が1に近いものばかりで、model.dict['wv']のベクトル値が小さい値ばかりの \n",
    "# ときは、学習回数が少ないと考えられます。\n",
    "# その場合、iterの値を大きくして、再度学習を行います。\n",
    "\n",
    "# 事前準備したword_listを使ってWord2Vecの学習実施\n",
    "model = word2vec.Word2Vec(word_list, size=100,min_count=5,window=5,iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color\n",
       "0    red\n",
       "1  Green\n",
       "2   Blue"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waka  =pd.read_csv('wakatchi.csv',header = None)\n",
    "waka = waka.rename(columns = ({0:'color'}))\n",
    "waka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'red'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waka['color'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'part_of_speech'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-cb152242c5b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#token = token.astype()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpartOfSpeech\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpart_of_speech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# 今回抽出するのは名詞だけとします。（もちろん他の品詞を追加、変更、除外可能です。）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpartOfSpeech\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mu'名詞'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'part_of_speech'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer(wakati=True)\n",
    "\n",
    "#with open('wakatchi.csv', encoding='utf-8') as f:\n",
    "#    reader = csv.reader(f)\n",
    "#    next(reader)\n",
    "#    for columns in reader:\n",
    "#        #for i in t.tokenize(columns[0]):\n",
    "#        for i in t.tokenize(columns[0]):\n",
    "#            print(i)\n",
    "#        print()\n",
    "        \n",
    "        \n",
    "for row in range(len(waka)):\n",
    "    val = waka['color'][row]\n",
    "    tokens = t.tokenize(val)\n",
    "    for token in tokens:\n",
    "        #token = token.astype()\n",
    "        partOfSpeech = token.part_of_speech.split(',')#[0]\n",
    "        # 今回抽出するのは名詞だけとします。（もちろん他の品詞を追加、変更、除外可能です。）\n",
    "        if partOfSpeech == u'名詞':\n",
    "        #    each_data.append(token.surface) # token.surfaceは表層形(語彙)です。詳しくはこちら...http://ailaby.com/janome/\n",
    "        # すべての形態素を含ませたいのであればif構文を外し、each_data.append(token.surface)のみ記載してください。\n",
    "            data.append(each_data)\n",
    "            each_data = []\n",
    "        #print(type(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "固有名詞\n",
    "サ変接続\n",
    "一般"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
